# Tinder Scraper Implementation Guide

This document provides detailed instructions for implementing, configuring, and deploying the Tinder Profile Scraper and Labeling Platform.

## Table of Contents

1. [System Requirements](#system-requirements)
2. [Detailed Installation](#detailed-installation)
3. [Configuration Options](#configuration-options)
4. [Google Sheets Setup](#google-sheets-setup)
5. [Tinder Account Setup](#tinder-account-setup)
6. [Deployment Options](#deployment-options)
7. [Scaling and Performance](#scaling-and-performance)
8. [Security Considerations](#security-considerations)
9. [Monitoring and Maintenance](#monitoring-and-maintenance)
10. [Backup and Recovery](#backup-and-recovery)

## System Requirements

### Minimum Requirements

- **CPU**: Dual-core processor, 2.0 GHz or higher
- **RAM**: 4 GB minimum (8 GB recommended)
- **Disk Space**: 10 GB free space (more needed as image collection grows)
- **Operating System**: Ubuntu 20.04+, Windows 10+, or macOS 10.15+
- **Network**: Stable internet connection (5 Mbps+)
- **Browser**: Chrome or Chromium 90+

### Recommended Requirements for Large-Scale Collection

- **CPU**: Quad-core processor, 3.0 GHz or higher
- **RAM**: 16 GB or more
- **Disk Space**: 500 GB+ SSD storage
- **Network**: 20 Mbps+ dedicated connection
- **Additional**: Consider using a server or cloud-based VM for continuous operation

## Detailed Installation

### Step 1: System Preparation

#### Ubuntu/Debian

```bash
# Update system packages
sudo apt update
sudo apt upgrade -y

# Install required system dependencies
sudo apt install -y python3 python3-pip python3-venv git

# Install Chrome and ChromeDriver
sudo apt install -y wget unzip
wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
sudo apt install -y ./google-chrome-stable_current_amd64.deb

# Install ChromeDriver
CHROME_VERSION=$(google-chrome --version | awk '{print $3}' | cut -d. -f1)
wget https://chromedriver.storage.googleapis.com/LATEST_RELEASE_${CHROME_VERSION} -O chromedriver_version
CHROMEDRIVER_VERSION=$(cat chromedriver_version)
wget https://chromedriver.storage.googleapis.com/${CHROMEDRIVER_VERSION}/chromedriver_linux64.zip
unzip chromedriver_linux64.zip
sudo mv chromedriver /usr/local/bin/
sudo chmod +x /usr/local/bin/chromedriver
```

#### macOS

```bash
# Install Homebrew if not already installed
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"

# Install Python and Git
brew install python git

# Install Chrome and ChromeDriver
brew install --cask google-chrome
brew install --cask chromedriver
```

#### Windows

1. Download and install Python 3.9+ from the [official website](https://www.python.org/downloads/)
2. During installation, check "Add Python to PATH"
3. Download and install Git from the [official website](https://git-scm.com/download/win)
4. Download and install Google Chrome from the [official website](https://www.google.com/chrome/)
5. Download ChromeDriver matching your Chrome version from the [official website](https://sites.google.com/chromium.org/driver/)
6. Extract the ChromeDriver executable and add its location to your system PATH

### Step 2: Project Setup

```bash
# Clone the repository
git clone https://github.com/yourusername/tinder-scraper.git
cd tinder-scraper

# Create and activate virtual environment
python -m venv venv

# On Windows
venv\Scripts\activate

# On macOS/Linux
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### Step 3: Environment Configuration

Create a `.env` file in the project root with the following contents:

```
# Tinder credentials
TINDER_EMAIL=your_tinder_email@example.com
TINDER_PASSWORD=your_tinder_password

# Google Sheets
GOOGLE_SHEETS_ID=your_google_sheets_id

# Application settings
OUTPUT_DIR=tinder_images
FLASK_DEBUG=False
PORT=5000

# Scraping settings
MIN_IMAGES_PER_PROFILE=5
PROFILES_PER_LOCATION=20
```

## Configuration Options

### Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `TINDER_EMAIL` | Email for Tinder account | N/A |
| `TINDER_PASSWORD` | Password for Tinder account | N/A |
| `GOOGLE_SHEETS_ID` | ID of Google Sheets document | N/A |
| `OUTPUT_DIR` | Directory to store scraped images | `tinder_images` |
| `FLASK_DEBUG` | Enable Flask debug mode | `False` |
| `PORT` | Port for the web server | `5000` |
| `MIN_IMAGES_PER_PROFILE` | Minimum images required per profile | `5` |
| `PROFILES_PER_LOCATION` | Profiles to scrape per location | `20` |
| `HEADLESS` | Run browser in headless mode | `True` |
| `CREDENTIALS_FILE` | Path to Google API credentials | `credentials.json` |

### Scraping Configuration

You can modify the scraping behavior by editing the `CONFIG` object in `tinder_scraper.py`:

```python
CONFIG = {
    'locations': [
        'New York, USA', 'Los Angeles, USA', 'Chicago, USA', 'Houston, USA',
        'London, UK', 'Manchester, UK', 'Paris, France', 'Berlin, Germany',
        # Add or remove locations as needed
    ],
    'wait_time_between_actions': (0.5, 2.0),  # Random wait time range in seconds
    'profiles_per_milestone': 5000,
    'min_images_per_profile': 5,
    'profiles_per_location': 20
}
```

## Google Sheets Setup

### Creating the Service Account

1. Go to the [Google Cloud Console](https://console.cloud.google.com/)
2. Create a new project or select an existing one
3. Navigate to "APIs & Services" > "Dashboard"
4. Click on "Enable APIs and Services"
5. Search for "Google Sheets API" and enable it
6. Go to "APIs & Services" > "Credentials"
7. Click "Create Credentials" > "Service Account"
8. Fill in the service account details and click "Create"
9. Grant the service account the "Editor" role
10. Click "Done"
11. Click on the service account you just created
12. Go to the "Keys" tab
13. Click "Add Key" > "Create new key"
14. Select "JSON" and click "Create"
15. Save the downloaded JSON file as `credentials.json` in your project directory

### Setting Up the Spreadsheet

1. Create a new Google Sheets document
2. Copy the Spreadsheet ID from the URL (the long string between `/d/` and `/edit`)
3. Share the spreadsheet with the service account email (with Editor permission)
4. Update the `GOOGLE_SHEETS_ID` in your `.env` file

The application will automatically create the necessary sheets with appropriate headers on first run.

## Tinder Account Setup

### Premium Account Requirements

This scraper requires a Tinder Premium account that includes the "Passport" feature to change location. The available premium tiers are:

- Tinder Plus
- Tinder Gold
- Tinder Platinum

Tinder Plus is sufficient as it includes the Passport feature.

### Account Setup Process

1. Create a Tinder account if you don't already have one
2. Verify your email and phone number
3. Complete your profile setup
4. Purchase a premium subscription through the app
5. Test the Passport feature manually to ensure it's working
6. Use the email and password for this account in your `.env` file

### Account Security and Privacy

- Consider using a dedicated account for scraping purposes
- Use a unique password not used for other services
- Enable two-factor authentication if possible
- Regularly check your account for any security notifications

## Deployment Options

### Local Development

```bash
# Start the Flask development server
python app.py
```

### Production Deployment with Gunicorn

```bash
# Install Gunicorn if not included in requirements.txt
pip install gunicorn

# Start the application with Gunicorn
gunicorn -w 4 -b 0.0.0.0:5000 app:app
```

### Docker Deployment

Create a `Dockerfile` in the project root:

```dockerfile
FROM python:3.9-slim

# Install Chrome and dependencies
RUN apt-get update && apt-get install -y \
    wget \
    gnupg \
    unzip \
    && wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | apt-key add - \
    && echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" >> /etc/apt/sources.list.d/google.list \
    && apt-get update \
    && apt-get install -y google-chrome-stable \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Install ChromeDriver
RUN CHROME_VERSION=$(google-chrome --version | awk '{print $3}' | cut -d. -f1) \
    && wget -q https://chromedriver.storage.googleapis.com/LATEST_RELEASE_${CHROME_VERSION} -O chromedriver_version \
    && CHROMEDRIVER_VERSION=$(cat chromedriver_version) \
    && wget -q https://chromedriver.storage.googleapis.com/${CHROMEDRIVER_VERSION}/chromedriver_linux64.zip \
    && unzip chromedriver_linux64.zip \
    && mv chromedriver /usr/local/bin/ \
    && chmod +x /usr/local/bin/chromedriver \
    && rm chromedriver_linux64.zip chromedriver_version

WORKDIR /app

# Copy requirements and install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Create output directory
RUN mkdir -p tinder_images

# Expose port
EXPOSE 5000

# Command to run the application
CMD ["gunicorn", "-w", "4", "-b", "0.0.0.0:5000", "app:app"]
```

Build and run the Docker container:

```bash
docker build -t tinder-scraper .
docker run -p 5000:5000 --env-file .env -v $(pwd)/tinder_images:/app/tinder_images tinder-scraper
```

### Cloud Deployment Options

The application can be deployed to various cloud providers:

#### Google Cloud Run

1. Build and push your Docker image to Google Container Registry
2. Deploy the container to Cloud Run with appropriate environment variables
3. Configure Cloud Run to provide sufficient memory (at least 2GB)
4. Set appropriate concurrency settings

#### Heroku

1. Add a `Procfile` with: `web: gunicorn app:app`
2. Set up the Heroku environment variables
3. Deploy to Heroku using Git or the Heroku CLI
4. Add Buildpacks for Chrome and ChromeDriver

#### AWS Elastic Beanstalk

1. Create a new Elastic Beanstalk environment
2. Deploy your application as a Docker container
3. Configure environment properties in the Elastic Beanstalk console
4. Set up appropriate instance type (at least t3.medium)

## Scaling and Performance

### Optimizing Scraping Speed

- Adjust the `wait_time_between_actions` parameter in the scraper
- Consider running multiple instances with different accounts
- Use a server with high-speed internet connection
- Optimize image downloading to use async operations

### Resource Management

- Implement image compression to reduce storage requirements
- Set up rotation policies for log files
- Configure proper caching for database queries
- Use a content delivery network (CDN) for static assets

### Concurrent Processing

For high-volume scraping, consider implementing a job queue system:

```python
# Example of adding Celery for background job processing
from celery import Celery

# Configure Celery
celery_app = Celery('tinder_scraper',
                    broker='redis://localhost:6379/0',
                    backend='redis://localhost:6379/0')

# Define task for scraping
@celery_app.task
def scrape_profiles_task(email, password, target_profiles, locations):
    # Scraping logic here
    pass

# Call the task
scrape_profiles_task.delay(email, password, target_profiles, locations)
```

## Security Considerations

### Protecting Credentials

- Never commit `.env` files or credential files to version control
- Use environment variables for all sensitive information
- Consider using a secrets management service in production

### API Security

- Implement proper authentication for all API endpoints
- Use HTTPS in production
- Rate limit API requests to prevent abuse
- Validate all input parameters

### Data Protection

- Implement access controls for the web interface
- Encrypt sensitive data at rest
- Anonymize personal information when exporting data
- Ensure compliance with data protection regulations

## Monitoring and Maintenance

### Application Monitoring

- Implement logging to a central service
- Set up alerts for critical errors
- Monitor system resource usage
- Track scraping progress and success rates

### Regular Maintenance Tasks

- Update dependencies regularly
- Check for ChromeDriver compatibility with Chrome updates
- Monitor Tinder API changes that might affect scraping
- Back up scraped data and labels

### Troubleshooting Common Issues

| Issue | Possible Cause | Solution |
|-------|--------------|----------|
| Selenium can't find elements | Tinder UI changes | Update XPath selectors |
| Frequent CAPTCHAs | Detected as a bot | Slow down scraping, add more human-like behavior |
| Google Sheets API errors | Rate limiting | Implement exponential backoff |
| Images not downloading | Network issues | Add retry logic for downloads |
| Chrome crashes | Memory issues | Increase container memory, restart browser regularly |

## Backup and Recovery

### Data Backup Strategy

1. Set up regular backups of the image directory
2. Export Google Sheets data periodically as CSV
3. Create database dumps for any additional storage
4. Store backups in a secure, offsite location

### Recovery Procedures

1. Restore from the latest backup
2. Verify data integrity
3. Re-authenticate with Google and Tinder if needed
4. Test the system thoroughly before resuming operation

### Disaster Recovery Plan

1. Document all configuration settings
2. Create step-by-step recovery instructions
3. Maintain a list of dependencies and versions
4. Test recovery procedures periodically

## Conclusion

By following this implementation guide, you should be able to set up, configure, and deploy the Tinder Profile Scraper and Labeling Platform successfully. Remember to use this tool responsibly and in compliance with all applicable terms of service and data protection regulations.
